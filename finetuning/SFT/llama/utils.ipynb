{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning llama3.2 1b - HF\n",
    "\n",
    "https://medium.com/@alexandros_chariton/how-to-fine-tune-llama-3-2-instruct-on-your-own-data-a-detailed-guide-e5f522f397d7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\YT40432\\.cache\\huggingface\\hub\\models--meta-llama--Llama-3.2-1B-Instruct\\snapshots\\9213176726f574b556790deb65791e0c5aa438b6\\config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\YT40432\\.cache\\huggingface\\hub\\models--meta-llama--Llama-3.2-1B-Instruct\\snapshots\\9213176726f574b556790deb65791e0c5aa438b6\\config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\YT40432\\.cache\\huggingface\\hub\\models--meta-llama--Llama-3.2-1B-Instruct\\snapshots\\9213176726f574b556790deb65791e0c5aa438b6\\model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ]\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-3.2-1B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at C:\\Users\\YT40432\\.cache\\huggingface\\hub\\models--meta-llama--Llama-3.2-1B-Instruct\\snapshots\\9213176726f574b556790deb65791e0c5aa438b6\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n",
      "loading file tokenizer.json from cache at C:\\Users\\YT40432\\.cache\\huggingface\\hub\\models--meta-llama--Llama-3.2-1B-Instruct\\snapshots\\9213176726f574b556790deb65791e0c5aa438b6\\tokenizer.json\n",
      "loading file tokenizer.model from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\YT40432\\.cache\\huggingface\\hub\\models--meta-llama--Llama-3.2-1B-Instruct\\snapshots\\9213176726f574b556790deb65791e0c5aa438b6\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\YT40432\\.cache\\huggingface\\hub\\models--meta-llama--Llama-3.2-1B-Instruct\\snapshots\\9213176726f574b556790deb65791e0c5aa438b6\\tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Device set to use cpu\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ANSWER:\n",
      "Deciding whether to move to Scandinavia can be a complex decision, and it's essential to consider several factors before making a decision. Here are some pros and cons to help you weigh your options:\n",
      "\n",
      "**Pros:**\n",
      "\n",
      "1. **High standard of living**: Scandinavia is known for its excellent quality of life, with high standards of education, healthcare, and social welfare.\n",
      "2. **Nature and environment**: The region is characterized by stunning natural landscapes, with many opportunities for outdoor activities like hiking, skiing, and fishing.\n",
      "3. **Cultural diversity**: Scandinavia is home to a diverse population, with many different cultures and languages to\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    device_map=\"auto\",\n",
    "    # device_map=device\n",
    ")\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Should I move to Scandinavia?\"},\n",
    "]\n",
    "outputs = pipe(\n",
    "    messages,\n",
    "    max_new_tokens=1024,\n",
    "    do_sample=True\n",
    ")\n",
    "print(\"\\nANSWER:\")\n",
    "print(outputs[0][\"generated_text\"][-1]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YT40432\\anaconda3\\envs\\research\\Lib\\site-packages\\accelerate\\utils\\modeling.py:1569: UserWarning: Current model requires 128 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import os \n",
    "\n",
    "root_dir = r\"C:\\Users\\YT40432\\Desktop\\lp\\research\\lucaspecina\\ai-basics\"\n",
    "models_dir = os.path.join(root_dir, \"models\\sft-llama\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the base model and tokenizer\n",
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float32, \n",
    "                                            #  device_map=\"auto\",\n",
    "                                             device_map=device,\n",
    "                                             ) # Must be float32 for MacBooks!\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 199 examples [00:00, 16582.89 examples/s]\n",
      "Map: 100%|██████████| 199/199 [00:00<00:00, 7960.35 examples/s]\n",
      "Map: 100%|██████████| 189/189 [00:00<00:00, 1500.00 examples/s]\n",
      "Map: 100%|██████████| 10/10 [00:00<00:00, 497.98 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Load the training dataset\n",
    "dataset = load_dataset(\"csv\", data_files=\"data/sarcasm.csv\", split=\"train\")\n",
    "\n",
    "# Define a function to apply the chat template\n",
    "def apply_chat_template(example):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": example['question']},\n",
    "        {\"role\": \"assistant\", \"content\": example['answer']}\n",
    "    ]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    return {\"prompt\": prompt}\n",
    "\n",
    "# Apply the chat template function to the dataset\n",
    "new_dataset = dataset.map(apply_chat_template)\n",
    "new_dataset = new_dataset.train_test_split(0.05) # Let's keep 5% of the data for testing\n",
    "\n",
    "# Tokenize the data\n",
    "def tokenize_function(example):\n",
    "    tokens = tokenizer(example['prompt'], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    # Set padding token labels to -100 to ignore them in loss calculation\n",
    "    tokens['labels'] = [\n",
    "        -100 if token == tokenizer.pad_token_id else token for token in tokens['input_ids']\n",
    "    ]\n",
    "    return tokens\n",
    "\n",
    "# Apply tokenize_function to each row\n",
    "tokenized_dataset = new_dataset.map(tokenize_function)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns(['question', 'answer', 'prompt'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\YT40432\\AppData\\Local\\Temp\\ipykernel_28492\\3522893616.py:20: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# Define training arguments\n",
    "model.train()\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=os.path.join(models_dir, \"results\"),\n",
    "    eval_strategy=\"steps\", # To evaluate during training\n",
    "    eval_steps=40,\n",
    "    logging_steps=40,\n",
    "    save_steps=150,\n",
    "    per_device_train_batch_size=2, # Adjust based on your hardware\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=2, # How many times to loop through the dataset\n",
    "    fp16=False, # Must be False for MacBooks\n",
    "    report_to=\"none\", # Here we can use something like tensorboard to see the training metrics\n",
    "    log_level=\"info\",\n",
    "    learning_rate=1e-5, # Would avoid larger values here\n",
    "    max_grad_norm=2 # Clipping the gradients is always a good idea\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 189\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 190\n",
      "  Number of trainable parameters = 1,235,814,400\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='190' max='190' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [190/190 55:53, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.576900</td>\n",
       "      <td>1.526454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.218000</td>\n",
       "      <td>1.091788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.584900</td>\n",
       "      <td>1.071233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.382900</td>\n",
       "      <td>1.060671</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 2\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 2\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./results\\checkpoint-150\n",
      "Configuration saved in ./results\\checkpoint-150\\config.json\n",
      "Configuration saved in ./results\\checkpoint-150\\generation_config.json\n",
      "Model weights saved in ./results\\checkpoint-150\\model.safetensors\n",
      "tokenizer config file saved in ./results\\checkpoint-150\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-150\\special_tokens_map.json\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./results\\checkpoint-190\n",
      "Configuration saved in ./results\\checkpoint-190\\config.json\n",
      "Configuration saved in ./results\\checkpoint-190\\generation_config.json\n",
      "Model weights saved in ./results\\checkpoint-190\\model.safetensors\n",
      "tokenizer config file saved in ./results\\checkpoint-190\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-190\\special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to ./fine-tuned-model\n",
      "Configuration saved in ./fine-tuned-model\\config.json\n",
      "Configuration saved in ./fine-tuned-model\\generation_config.json\n",
      "Model weights saved in ./fine-tuned-model\\model.safetensors\n",
      "tokenizer config file saved in ./fine-tuned-model\\tokenizer_config.json\n",
      "Special tokens file saved in ./fine-tuned-model\\special_tokens_map.json\n",
      "tokenizer config file saved in ./fine-tuned-model\\tokenizer_config.json\n",
      "Special tokens file saved in ./fine-tuned-model\\special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./fine-tuned-model\\\\tokenizer_config.json',\n",
       " './fine-tuned-model\\\\special_tokens_map.json',\n",
       " './fine-tuned-model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model and tokenizer\n",
    "trainer.save_model(os.path.join(models_dir, \"fine-tuned-model\")),\n",
    "tokenizer.save_pretrained(os.path.join(models_dir, \"fine-tuned-model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ANSWER:\n",
      "Absolutely! The land of the midnight sun, where the people are all about sustainability and cozy sweaters. You'll love it here. And the food? It's all about the local, organic, gluten-free cuisine. Your taste buds will thank you. Plus, the people are all so... chill. You'll fit right in. Give it a moment or two, and you'll be hooked. Trust me. Scandinavia is the new Sweden. You'll love it here. Next, you should move to Iceland. It's basically the same idea, but with more geothermal hot springs. Worth a shot?\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import os \n",
    "import torch \n",
    "\n",
    "root_dir = r\"C:\\Users\\YT40432\\Desktop\\lp\\research\\lucaspecina\\ai-basics\"\n",
    "models_dir = os.path.join(root_dir, \"models\\sft-llama\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_id = os.path.join(models_dir, \"fine-tuned-model\")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    device_map=\"auto\",\n",
    "    # device_map=device,\n",
    ")\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Should I move to Scandinavia?\"},\n",
    "]\n",
    "outputs_sft = pipe(\n",
    "    messages,\n",
    "    max_new_tokens=1024\n",
    ")\n",
    "print('\\nANSWER:')\n",
    "print(outputs_sft[0][\"generated_text\"][-1]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolutely! The land of the midnight sun, where the only traffic jam is at the grocery store. And the food? Absolutely delicious. You'll never go hungry. Plus, the people are all so friendly and welcoming. You'll love it here. Trust me. I've lived here for 5 years now and I'm still not tired of it. Give it a moment or two. You might just fall in love. Scandinavia: it's a real thing. You can see it in the way the lights flicker. It’s real. Scandinavia: it's real. Scandinavia: it's real. Scandinavia: it\n"
     ]
    }
   ],
   "source": [
    "print(outputs_sft[0][\"generated_text\"][-1]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action:\n",
      "{{\n",
      "  \"name\": \"web_search\",\n",
      "  \"arguments\": {{\"query\": \"simple explanation of supervised learning\"}}\n",
      "}}\n",
      "Observation: \"Supervised learning is a type of machine learning where the algorithm is trained on labeled data to learn patterns and make predictions on new, unseen data.\"\n",
      "\n",
      "Action:\n",
      "{{\n",
      "  \"name\": \"web_search\",\n",
      "  \"arguments\": {{\"query\": \"supervised learning in simple terms\"}}\n",
      "}}\n",
      "Observation: \"Sup,\n"
     ]
    }
   ],
   "source": [
    "print(\"Action:\\r\\n{{\\r\\n  \\\"name\\\": \\\"web_search\\\",\\r\\n  \\\"arguments\\\": {{\\\"query\\\": \\\"simple explanation of supervised learning\\\"}}\\r\\n}}\\r\\nObservation: \\\"Supervised learning is a type of machine learning where the algorithm is trained on labeled data to learn patterns and make predictions on new, unseen data.\\\"\\r\\n\\r\\nAction:\\r\\n{{\\r\\n  \\\"name\\\": \\\"web_search\\\",\\r\\n  \\\"arguments\\\": {{\\\"query\\\": \\\"supervised learning in simple terms\\\"}}\\r\\n}}\\r\\nObservation: \\\"Sup,\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
