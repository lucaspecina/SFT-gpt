{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c85ae4c4",
   "metadata": {},
   "source": [
    "# Download base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3784ddb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_model import GPTModel\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"drop_rate\": 0.0,        # Dropout rate\n",
    "    \"qkv_bias\": True         # Query-key-value bias\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-medium (355M)\"\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "model = GPTModel(BASE_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc6fae1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GPT2 355M model on cuda...\n",
      "File already exists and is up-to-date: gpt2\\355M\\checkpoint\n",
      "File already exists and is up-to-date: gpt2\\355M\\encoder.json\n",
      "File already exists and is up-to-date: gpt2\\355M\\hparams.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.ckpt.data-00000-of-00001: 100%|██████████| 1.42G/1.42G [36:30<00:00, 648kiB/s]\n",
      "model.ckpt.index: 100%|██████████| 10.4k/10.4k [00:00<00:00, 2.60MiB/s]\n",
      "model.ckpt.meta: 100%|██████████| 927k/927k [00:01<00:00, 593kiB/s]\n",
      "vocab.bpe: 100%|██████████| 456k/456k [00:01<00:00, 439kiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vocab_size': 50257, 'context_length': 1024, 'emb_dim': 1024, 'n_layers': 24, 'n_heads': 16, 'drop_rate': 0.0, 'qkv_bias': True}\n"
     ]
    }
   ],
   "source": [
    "from gpt2_base_download import download_and_load_gpt2\n",
    "import torch \n",
    "from gpt_model import load_weights_into_gpt, text_to_token_ids, token_ids_to_text, generate\n",
    "import tiktoken\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Loading GPT2 {model_size} model on {device}...\")\n",
    "settings, params = download_and_load_gpt2(model_size=model_size, models_dir=\"gpt2\")\n",
    "base_cfg = {\n",
    "    \"vocab_size\": settings[\"n_vocab\"],\n",
    "    \"context_length\": settings[\"n_ctx\"],\n",
    "    \"emb_dim\": settings[\"n_embd\"],\n",
    "    \"n_layers\": settings[\"n_layer\"],\n",
    "    \"n_heads\": settings[\"n_head\"],\n",
    "    \"drop_rate\": settings.get(\"resid_pdrop\", 0.0),\n",
    "    \"qkv_bias\": True\n",
    "}\n",
    "print(base_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf6726a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8da52039",
   "metadata": {},
   "source": [
    "# Prompt base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcebd261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "--------------------------------------------------\n",
      "Loading model: gpt2-medium (355M)\n",
      "File already exists and is up-to-date: C:\\Users\\YT40432\\Desktop\\lp\\research\\lucaspecina\\ai-basics\\models\\gpt2\\355M\\checkpoint\n",
      "File already exists and is up-to-date: C:\\Users\\YT40432\\Desktop\\lp\\research\\lucaspecina\\ai-basics\\models\\gpt2\\355M\\encoder.json\n",
      "File already exists and is up-to-date: C:\\Users\\YT40432\\Desktop\\lp\\research\\lucaspecina\\ai-basics\\models\\gpt2\\355M\\hparams.json\n",
      "File already exists and is up-to-date: C:\\Users\\YT40432\\Desktop\\lp\\research\\lucaspecina\\ai-basics\\models\\gpt2\\355M\\model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: C:\\Users\\YT40432\\Desktop\\lp\\research\\lucaspecina\\ai-basics\\models\\gpt2\\355M\\model.ckpt.index\n",
      "File already exists and is up-to-date: C:\\Users\\YT40432\\Desktop\\lp\\research\\lucaspecina\\ai-basics\\models\\gpt2\\355M\\model.ckpt.meta\n",
      "File already exists and is up-to-date: C:\\Users\\YT40432\\Desktop\\lp\\research\\lucaspecina\\ai-basics\\models\\gpt2\\355M\\vocab.bpe\n",
      "Model loaded successfully.\n",
      "--------------------------------------------------\n",
      "Prompt: >>>\n",
      "Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.\n",
      "<<<\n",
      "\n",
      "Generating 50 tokens (temperature=0.7, top_k=50)...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Encountered text corresponding to disallowed special token '<|endoftext|>'.\nIf you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.\nIf you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.\nTo disable this check for all special tokens, pass `disallowed_special=()`.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 76\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mGenerating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMAX_NEW_TOKENS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m tokens (temperature=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTEMPERATURE\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, top_k=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTOP_K\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     68\u001b[39m torch.manual_seed(\u001b[32m123\u001b[39m)  \u001b[38;5;66;03m# For reproducibility\u001b[39;00m\n\u001b[32m     69\u001b[39m token_ids = generate(\n\u001b[32m     70\u001b[39m     model=model,\n\u001b[32m     71\u001b[39m     idx=encoded_prompt,\n\u001b[32m     72\u001b[39m     max_new_tokens=MAX_NEW_TOKENS,\n\u001b[32m     73\u001b[39m     context_size=BASE_CONFIG[\u001b[33m\"\u001b[39m\u001b[33mcontext_length\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     74\u001b[39m     temperature=TEMPERATURE,\n\u001b[32m     75\u001b[39m     top_k=TOP_K,\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m     eos_id=\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m<|endoftext|>\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m     77\u001b[39m )\n\u001b[32m     79\u001b[39m \u001b[38;5;66;03m# Decode and print the generated text\u001b[39;00m\n\u001b[32m     80\u001b[39m decoded_text = token_ids_to_text(token_ids[\u001b[32m0\u001b[39m], tokenizer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\YT40432\\anaconda3\\envs\\research\\Lib\\site-packages\\tiktoken\\core.py:117\u001b[39m, in \u001b[36mEncoding.encode\u001b[39m\u001b[34m(self, text, allowed_special, disallowed_special)\u001b[39m\n\u001b[32m    115\u001b[39m         disallowed_special = \u001b[38;5;28mfrozenset\u001b[39m(disallowed_special)\n\u001b[32m    116\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m match := _special_token_regex(disallowed_special).search(text):\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m         \u001b[43mraise_disallowed_special_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmatch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[38;5;66;03m# https://github.com/PyO3/pyo3/pull/3632\u001b[39;00m\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(allowed_special, \u001b[38;5;28mfrozenset\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\YT40432\\anaconda3\\envs\\research\\Lib\\site-packages\\tiktoken\\core.py:400\u001b[39m, in \u001b[36mraise_disallowed_special_token\u001b[39m\u001b[34m(token)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mraise_disallowed_special_token\u001b[39m(token: \u001b[38;5;28mstr\u001b[39m) -> NoReturn:\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    401\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEncountered text corresponding to disallowed special token \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtoken\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    402\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIf you want this text to be encoded as a special token, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    403\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mpass it to `allowed_special`, e.g. `allowed_special=\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtoken\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m, ...\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[33m`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    404\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIf you want this text to be encoded as normal text, disable the check for this token \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    405\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mby passing `disallowed_special=(enc.special_tokens_set - \u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtoken\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[33m)`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    406\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mTo disable this check for all special tokens, pass `disallowed_special=()`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    407\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Encountered text corresponding to disallowed special token '<|endoftext|>'.\nIf you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.\nIf you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.\nTo disable this check for all special tokens, pass `disallowed_special=()`.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import tiktoken\n",
    "\n",
    "# Import from local files in this folder\n",
    "from gpt_model import (\n",
    "    GPTModel,\n",
    "    generate,\n",
    "    load_weights_into_gpt,\n",
    "    text_to_token_ids,\n",
    "    token_ids_to_text\n",
    ")\n",
    "from gpt2_base_download import download_and_load_gpt2\n",
    "\n",
    "# Configuration\n",
    "CHOOSE_MODEL = \"gpt2-medium (355M)\"  # Or choose \"gpt2-small (124M)\", \"gpt2-large (774M)\", \"gpt2-xl (1558M)\"\n",
    "MODELS_DIR = r\"C:\\Users\\YT40432\\Desktop\\lp\\research\\lucaspecina\\ai-basics\\models\\gpt2\" # Adjust this path if your models are stored elsewhere\n",
    "PROMPT = \"Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.\"\n",
    "MAX_NEW_TOKENS = 50\n",
    "TEMPERATURE = 0.7\n",
    "TOP_K = 50\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(50*\"-\")\n",
    "\n",
    "# Define model config based on chosen model\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"drop_rate\": 0.0,\n",
    "    \"qkv_bias\": True\n",
    "}\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "if CHOOSE_MODEL not in model_configs:\n",
    "    raise ValueError(f\"Invalid model size. Choose from {list(model_configs.keys())}\")\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "# Download and load model weights\n",
    "print(f\"Loading model: {CHOOSE_MODEL}\")\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "settings, params = download_and_load_gpt2(model_size=model_size, models_dir=MODELS_DIR)\n",
    "\n",
    "# Instantiate the model\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "print(\"Model loaded successfully.\")\n",
    "print(50*\"-\")\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Tokenize the prompt\n",
    "encoded_prompt = text_to_token_ids(PROMPT, tokenizer).to(device)\n",
    "\n",
    "# Generate text\n",
    "print(f\"Prompt: >>>\\n{PROMPT}\\n<<<\")\n",
    "print(f\"\\nGenerating {MAX_NEW_TOKENS} tokens (temperature={TEMPERATURE}, top_k={TOP_K})...\")\n",
    "torch.manual_seed(123)  # For reproducibility\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=encoded_prompt,\n",
    "    max_new_tokens=MAX_NEW_TOKENS,\n",
    "    context_size=BASE_CONFIG[\"context_length\"],\n",
    "    temperature=TEMPERATURE,\n",
    "    top_k=TOP_K,\n",
    "    eos_id=tokenizer.encode(\"<|endoftext|>\")[0]\n",
    ")\n",
    "\n",
    "# Decode and print the generated text\n",
    "decoded_text = token_ids_to_text(token_ids[0], tokenizer)\n",
    "print(f\"\\nGenerated text: >>>\\n{decoded_text}\\n<<<\")\n",
    "print(50*\"-\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acda302",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e27f36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f3f83194-82b9-4478-9550-5ad793467bd0",
   "metadata": {},
   "source": [
    "# Load And Use Finetuned Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466b564e-4fd5-4d76-a3a1-63f9f0993b7e",
   "metadata": {},
   "source": [
    "This notebook contains minimal code to load the finetuned model that was instruction finetuned and saved in chapter 7 via [ch07.ipynb](ch07.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd80e5f5-0f79-4a6c-bf31-2026e7d30e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.6.0\n",
      "torch version: 2.6.0+cu124\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\n",
    "    \"tiktoken\",    # Tokenizer\n",
    "    \"torch\",       # Deep learning library\n",
    "]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed86d6b7-f32d-4601-b585-a2ea3dbf7201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find 'gpt2-medium355M-sft.pth'.\n",
      "Run the `ch07.ipynb` notebook to finetune and save the finetuned model.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "finetuned_model_path = Path(\"gpt2-medium355M-sft.pth\")\n",
    "if not finetuned_model_path.exists():\n",
    "    print(\n",
    "        f\"Could not find '{finetuned_model_path}'.\\n\"\n",
    "        \"Run the `ch07.ipynb` notebook to finetune and save the finetuned model.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb02584a-5e31-45d5-8377-794876907bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from previous_chapters import GPTModel\n",
    "\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"drop_rate\": 0.0,        # Dropout rate\n",
    "    \"qkv_bias\": True         # Query-key-value bias\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-medium (355M)\"\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "model = GPTModel(BASE_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1ccf2b7-176e-4cfd-af7a-53fb76010b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model.load_state_dict(torch.load(\n",
    "    \"gpt2-medium355M-sft.pth\",\n",
    "    map_location=torch.device(\"cpu\"),\n",
    "    weights_only=True\n",
    "))\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1fd174e-9555-46c5-8780-19b0aa4f26e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a4c0129-efe5-46e9-bb90-ba08d407c1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Below is an instruction that describes a task. Write a response \n",
    "that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e26862c-10b5-4a0f-9dd6-b6ddbad2fc3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The meal is cooked every day by the chef.\n"
     ]
    }
   ],
   "source": [
    "from previous_chapters import (\n",
    "    generate,\n",
    "    text_to_token_ids,\n",
    "    token_ids_to_text\n",
    ")\n",
    "\n",
    "def extract_response(response_text, input_text):\n",
    "    return response_text[len(input_text):].replace(\"### Response:\", \"\").strip()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(prompt, tokenizer),\n",
    "    max_new_tokens=35,\n",
    "    context_size=BASE_CONFIG[\"context_length\"],\n",
    "    eos_id=50256\n",
    ")\n",
    "\n",
    "response = token_ids_to_text(token_ids, tokenizer)\n",
    "response = extract_response(response, prompt)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
